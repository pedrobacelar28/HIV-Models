"""
Pr√©-treinador MLM (Masked Language Modeling) para ESM-C em epitopos
Estrat√©gia: Mascarar amino√°cidos aleat√≥rios e treinar o modelo para prever os tokens originais
"""

from __future__ import annotations
import json, random, datetime, os
from pathlib import Path
from typing import Dict, Tuple, Any, List
import numpy as np
import torch
from torch import nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import wandb
from models import create_model, get_tokenizer
from copy import deepcopy


class MLMDataset(Dataset):
    """Dataset para Masked Language Modeling"""
    
    def __init__(self, sequences: List[str], tokenizer, max_length: int = 1200, 
                 mlm_probability: float = 0.15):
        self.sequences = sequences
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.mlm_probability = mlm_probability
        
        # Obter tokens especiais
        if hasattr(tokenizer, 'mask_token_id'):
            self.mask_token_id = tokenizer.mask_token_id
        elif hasattr(tokenizer, '_token_to_id') and '<mask>' in tokenizer._token_to_id:
            self.mask_token_id = tokenizer._token_to_id['<mask>']
        else:
            # Para ESM-C, usar um valor padr√£o ou descobrir dinamicamente
            self.mask_token_id = 32  # Valor t√≠pico para <mask> em ESM-C
        
        # Descobrir range de tokens v√°lidos para amino√°cidos
        self._discover_valid_tokens()
    
    def _discover_valid_tokens(self):
        """Descobre quais tokens s√£o v√°lidos para mascaramento (amino√°cidos)"""
        # Amino√°cidos padr√£o
        amino_acids = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 
                      'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']
        
        self.valid_tokens = set()
        
        # Para ESM-C
        if hasattr(self.tokenizer, '_token_to_id'):
            for aa in amino_acids:
                if aa in self.tokenizer._token_to_id:
                    self.valid_tokens.add(self.tokenizer._token_to_id[aa])
        # Para ESM2 (transformers)
        elif hasattr(self.tokenizer, 'vocab'):
            for aa in amino_acids:
                if aa in self.tokenizer.vocab:
                    self.valid_tokens.add(self.tokenizer.vocab[aa])
        else:
            # Fallback: assumir range t√≠pico de tokens de amino√°cidos
            self.valid_tokens = set(range(4, 24))  # Range t√≠pico para ESM
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        sequence = self.sequences[idx]
        
        # Tokenizar sequ√™ncia
        if hasattr(self.tokenizer, '__call__'):
            # ESM2 (transformers)
            encoded = self.tokenizer(
                sequence,
                truncation=True,
                padding="max_length",
                max_length=self.max_length,
                return_tensors="pt"
            )
            input_ids = encoded["input_ids"].squeeze(0)
            attention_mask = encoded["attention_mask"].squeeze(0)
        else:
            # ESM-C
            tokens = self.tokenizer.encode(sequence)
            # Pad ou truncar
            if len(tokens) > self.max_length:
                tokens = tokens[:self.max_length]
            else:
                tokens = tokens + [0] * (self.max_length - len(tokens))  # 0 = pad
            
            input_ids = torch.tensor(tokens, dtype=torch.long)
            attention_mask = torch.ones_like(input_ids)
            attention_mask[input_ids == 0] = 0  # Mask padding tokens
        
        # Criar labels para MLM (inicialmente c√≥pia de input_ids)
        labels = input_ids.clone()
        
        # Aplicar mascaramento
        masked_input_ids, labels = self._apply_masking(input_ids, labels, attention_mask)
        
        return masked_input_ids, attention_mask, labels
    
    def _apply_masking(self, input_ids, labels, attention_mask):
        """Aplica mascaramento MLM aos tokens"""
        masked_input_ids = input_ids.clone()
        
        # S√≥ mascarar tokens v√°lidos (amino√°cidos) e que n√£o sejam padding
        valid_positions = []
        for i, token_id in enumerate(input_ids):
            if (attention_mask[i] == 1 and  # N√£o √© padding
                token_id.item() in self.valid_tokens):  # √â amino√°cido
                valid_positions.append(i)
        
        if len(valid_positions) == 0:
            # Se n√£o h√° posi√ß√µes v√°lidas, retornar sem mascaramento
            labels.fill_(-100)  # -100 √© ignorado na loss
            return masked_input_ids, labels
        
        # Calcular quantos tokens mascarar
        num_to_mask = max(1, int(len(valid_positions) * self.mlm_probability))
        
        # Selecionar posi√ß√µes aleat√≥rias para mascarar
        positions_to_mask = random.sample(valid_positions, num_to_mask)
        
        # Aplicar estrat√©gia de mascaramento (80% <mask>, 10% random, 10% unchanged)
        for pos in positions_to_mask:
            rand = random.random()
            if rand < 0.8:
                # 80%: substituir por <mask>
                masked_input_ids[pos] = self.mask_token_id
            elif rand < 0.9:
                # 10%: substituir por token aleat√≥rio de amino√°cido
                random_aa_token = random.choice(list(self.valid_tokens))
                masked_input_ids[pos] = random_aa_token
            # 10%: manter original (j√° est√° correto)
        
        # Configurar labels: -100 para posi√ß√µes n√£o mascaradas
        labels[~torch.isin(torch.arange(len(labels)), torch.tensor(positions_to_mask))] = -100
        
        return masked_input_ids, labels


class ESMCForMLM(nn.Module):
    """Wrapper do ESM-C para Masked Language Modeling"""
    
    def __init__(self, base_model: str = "esmc_300m"):
        super().__init__()
        
        from esm.pretrained import load_local_model
        self.esmc = load_local_model(base_model)
        
        # Cabe√ßa MLM usando as mesmas dimens√µes do modelo base
        d_model = self.esmc.embed.embedding_dim
        vocab_size = self.esmc.embed.num_embeddings
        
        self.mlm_head = nn.Linear(d_model, vocab_size)
        
        # Inicializar pesos da cabe√ßa MLM
        nn.init.normal_(self.mlm_head.weight, std=0.02)
        nn.init.zeros_(self.mlm_head.bias)
    
    def forward(self, input_ids, labels=None):
        # Forward pass pelo modelo base
        outputs = self.esmc(sequence_tokens=input_ids)
        
        # Obter representa√ß√µes de todos os tokens
        hidden_states = outputs.embeddings  # [batch_size, seq_len, hidden_size]
        
        # Aplicar cabe√ßa MLM
        prediction_scores = self.mlm_head(hidden_states)  # [batch_size, seq_len, vocab_size]
        
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()  # ignora -100 automaticamente
            loss = loss_fct(prediction_scores.view(-1, prediction_scores.size(-1)), 
                           labels.view(-1))
        
        return {
            'loss': loss,
            'logits': prediction_scores,
            'hidden_states': hidden_states
        }


class PretrainerESMC:
    """Pr√©-treinador MLM para ESM-C"""
    
    def __init__(
        self,
        sequences: List[str],
        artifacts_path: str,
        lr: float = 5e-5,
        weight_decay: float = 0.01,
        batch_size: int = 16,
        max_length: int = 512,  # Menor para epitopos
        epochs: int = 10,
        save_interval: int = 2,
        base_model: str = "esmc_300m",
        mlm_probability: float = 0.15,
        warmup_steps: int = 1000,
        project: str = "esmc-pretraining",
        entity: str = None,
        run_name: str = None,
        seed: int = 42,
        resume_from_checkpoint: str = None,  # NOVO: caminho para checkpoint
        **kwargs
    ):
        # Configura√ß√£o
        self.sequences = sequences
        self.lr = lr
        self.weight_decay = weight_decay
        self.batch_size = batch_size
        self.max_length = max_length
        self.epochs = epochs
        self.save_interval = save_interval
        self.base_model = base_model
        self.mlm_probability = mlm_probability
        self.warmup_steps = warmup_steps
        self.project = project
        self.entity = entity
        self.seed = seed
        self.global_step = 0
        
        # NOVO: Para continuar pr√©-treinamento
        self.resume_from_checkpoint = resume_from_checkpoint
        self.start_epoch = 1
        
        # Reprodutibilidade
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        
        # Device
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Paths
        time_stamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_name = run_name if run_name is not None else f"pretrain_{base_model}_{time_stamp}"
        self.artifacts_path = Path(artifacts_path) / "pretraining" / self.run_name
        self.artifacts_path.mkdir(parents=True, exist_ok=True)
        
        # W&B
        wandb.init(
            project=project,
            entity=entity,
            name=self.run_name,
            config={
                "num_sequences": len(sequences),
                "lr": lr,
                "weight_decay": weight_decay,
                "batch_size": batch_size,
                "max_length": max_length,
                "epochs": epochs,
                "base_model": base_model,
                "mlm_probability": mlm_probability,
                "warmup_steps": warmup_steps,
                "seed": seed,
            },
            reinit="finish_previous",
        )
        
        # Inicializar componentes
        self.tokenizer = get_tokenizer(self.base_model)
        self.setup_data()
        self.setup_model()
        self.setup_optimizer()
        
        # NOVO: Carregar checkpoint se especificado
        self.load_checkpoint()
        
        print(f"üöÄ Pr√©-treinador inicializado")
        print(f"   üìä {len(self.sequences)} sequ√™ncias")
        print(f"   üß¨ Modelo: {self.base_model}")
        print(f"   üì± Device: {self.device}")
        print(f"   üíæ Artifacts: {self.artifacts_path}")
        
        if self.resume_from_checkpoint:
            print(f"   üîÑ Continuando de: √©poca {self.start_epoch}")
        else:
            print(f"   üÜï Novo treinamento")
    
    def setup_data(self):
        """Configurar dataset e dataloader"""
        # Filtrar sequ√™ncias muito curtas (< 5 amino√°cidos)
        filtered_sequences = [seq for seq in self.sequences if len(seq) >= 5]
        
        print(f"üìã Sequ√™ncias filtradas: {len(filtered_sequences)}/{len(self.sequences)}")
        
        # Dataset
        self.dataset = MLMDataset(
            sequences=filtered_sequences,
            tokenizer=self.tokenizer,
            max_length=self.max_length,
            mlm_probability=self.mlm_probability
        )
        
        # DataLoader
        self.dataloader = DataLoader(
            self.dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=True
        )
        
        print(f"üîÑ DataLoader: {len(self.dataloader)} batches")
    
    def setup_model(self):
        """Configurar modelo MLM"""
        self.model = ESMCForMLM(base_model=self.base_model).to(self.device)
        
        # Contar par√¢metros
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"üß† Modelo carregado:")
        print(f"   üìä Total params: {total_params:,}")
        print(f"   üéØ Trainable params: {trainable_params:,}")
    
    def setup_optimizer(self):
        """Configurar otimizador e scheduler"""
        from transformers.optimization import get_cosine_schedule_with_warmup
        
        # Otimizador
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.lr,
            weight_decay=self.weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # Scheduler
        total_steps = len(self.dataloader) * self.epochs
        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=self.warmup_steps,
            num_training_steps=total_steps
        )
        
        print(f"‚öôÔ∏è  Optimizer: AdamW (lr={self.lr}, wd={self.weight_decay})")
        print(f"üìà Scheduler: Cosine w/ warmup ({self.warmup_steps} steps)")
        print(f"üéØ Total steps: {total_steps}")
    
    def load_checkpoint(self):
        """Carregar checkpoint para continuar pr√©-treinamento"""
        if not self.resume_from_checkpoint:
            return
            
        if not os.path.exists(self.resume_from_checkpoint):
            print(f"‚ùå Checkpoint n√£o encontrado: {self.resume_from_checkpoint}")
            return
            
        print(f"üîÑ Carregando checkpoint: {self.resume_from_checkpoint}")
        
        checkpoint = torch.load(self.resume_from_checkpoint, map_location=self.device)
        
        # Carregar estado do modelo
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # Carregar estado do otimizador
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # Carregar estado do scheduler
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # Carregar outras vari√°veis
        self.start_epoch = checkpoint['epoch'] + 1  # Pr√≥xima √©poca
        self.global_step = checkpoint['global_step']
        
        print(f"‚úÖ Checkpoint carregado!")
        print(f"   üìÖ Continuando da √©poca {self.start_epoch}")
        print(f"   üéØ Global step: {self.global_step}")
        print(f"   üìä Loss anterior: {checkpoint['metrics']['avg_loss']:.4f}")
    
    def train_epoch(self) -> Dict[str, float]:
        """Treinar uma √©poca"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(
            self.dataloader,
            desc=f"Pr√©-treinamento",
            unit="batch"
        )
        
        for batch in progress_bar:
            input_ids, attention_mask, labels = [b.to(self.device) for b in batch]
            
            # Forward pass
            outputs = self.model(input_ids=input_ids, labels=labels)
            loss = outputs['loss']
            
            # Backward pass
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # Update
            self.optimizer.step()
            self.scheduler.step()
            self.optimizer.zero_grad()
            
            # Logging
            total_loss += loss.item()
            num_batches += 1
            self.global_step += 1
            
            # Update progress bar
            current_lr = self.scheduler.get_last_lr()[0]
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{total_loss/num_batches:.4f}',
                'lr': f'{current_lr:.2e}'
            })
            
            # Log para W&B
            if self.global_step % 50 == 0:
                wandb.log({
                    'train/loss': loss.item(),
                    'train/learning_rate': current_lr,
                    'train/step': self.global_step
                }, step=self.global_step)
        
        avg_loss = total_loss / num_batches
        return {'avg_loss': avg_loss}
    
    def save_checkpoint(self, epoch: int, metrics: Dict[str, float]):
        """Salvar checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'global_step': self.global_step,
            'metrics': metrics,
            'config': {
                'base_model': self.base_model,
                'max_length': self.max_length,
                'mlm_probability': self.mlm_probability,
            }
        }
        
        # Salvar checkpoint regular
        checkpoint_path = self.artifacts_path / f"checkpoint_epoch_{epoch}.pt"
        torch.save(checkpoint, checkpoint_path)
        
        # Salvar modelo pr√©-treinado (s√≥ os pesos do backbone)
        pretrained_path = self.artifacts_path / f"pretrained_esmc_epoch_{epoch}.pt"
        torch.save(self.model.esmc.state_dict(), pretrained_path)
        
        print(f"üíæ Checkpoint salvo: {checkpoint_path.name}")
        print(f"üß¨ Modelo pr√©-treinado salvo: {pretrained_path.name}")
    
    def run(self):
        """Executar pr√©-treinamento"""
        print(f"\nüöÄ Iniciando pr√©-treinamento ({self.epochs} √©pocas)")
        print("="*50)
        
        # Verificar se h√° √©pocas para executar
        if self.start_epoch > self.epochs:
            print(f"‚ö†Ô∏è  Aviso: √âpoca inicial ({self.start_epoch}) > √©pocas totais ({self.epochs})")
            print(f"üí° Nada a executar. Use --pretrain-epochs {self.start_epoch + 5} para continuar")
            
            # Retornar o melhor modelo existente se dispon√≠vel
            best_pretrained_path = self.artifacts_path / "best_pretrained_esmc.pt"
            if best_pretrained_path.exists():
                return str(best_pretrained_path)
            else:
                # Tentar encontrar o √∫ltimo modelo
                last_epoch = self.start_epoch - 1
                last_pretrained_path = self.artifacts_path / f"pretrained_esmc_epoch_{last_epoch}.pt"
                if last_pretrained_path.exists():
                    print(f"üéØ Usando modelo da √©poca {last_epoch}")
                    return str(last_pretrained_path)
                else:
                    raise RuntimeError("Nenhum modelo pr√©-treinado encontrado para continuar")
        
        # Inicializar metrics para caso nenhuma √©poca seja executada
        metrics = {'avg_loss': 0.0}
        epochs_executed = 0
        
        for epoch in range(self.start_epoch, self.epochs + 1):
            print(f"\nüìÖ √âpoca {epoch}/{self.epochs}")
            
            # Treinar
            metrics = self.train_epoch()
            epochs_executed += 1
            
            # Log √©poca
            wandb.log({
                'epoch': epoch,
                'train/epoch_loss': metrics['avg_loss']
            }, step=self.global_step)
            
            print(f"   üìä Loss m√©dio: {metrics['avg_loss']:.4f}")
            
            # Salvar checkpoint
            if epoch % self.save_interval == 0:
                self.save_checkpoint(epoch, metrics)
        
        # Salvar modelo final apenas se executou pelo menos uma √©poca
        if epochs_executed > 0:
            print(f"\n‚úÖ Pr√©-treinamento conclu√≠do!")
            self.save_checkpoint(self.epochs, metrics)
            
            # Salvar melhor modelo como "best"
            best_pretrained_path = self.artifacts_path / "best_pretrained_esmc.pt"
            final_pretrained_path = self.artifacts_path / f"pretrained_esmc_epoch_{self.epochs}.pt"
            
            import shutil
            shutil.copy2(final_pretrained_path, best_pretrained_path)
            print(f"üèÜ Melhor modelo salvo: {best_pretrained_path.name}")
        else:
            print(f"\n‚úÖ Nenhuma √©poca executada (j√° completado)")
            best_pretrained_path = self.artifacts_path / "best_pretrained_esmc.pt"
        
        wandb.finish()
        
        return str(best_pretrained_path)


def load_sequences_from_files(pos_file: str, neg_file: str) -> List[str]:
    """Carregar sequ√™ncias dos arquivos de dados"""
    sequences = []
    
    # Carregar positivos
    if os.path.exists(pos_file):
        with open(pos_file, 'r') as f:
            sequences.extend([line.strip() for line in f if line.strip()])
    
    # Carregar negativos
    if os.path.exists(neg_file):
        with open(neg_file, 'r') as f:
            sequences.extend([line.strip() for line in f if line.strip()])
    
    # Remover duplicatas
    sequences = list(set(sequences))
    
    print(f"üìä Carregadas {len(sequences)} sequ√™ncias √∫nicas")
    return sequences


def list_pretraining_checkpoints(dataset: str, model: str) -> List[Path]:
    """Listar checkpoints de pr√©-treinamento dispon√≠veis"""
    base_path = Path(__file__).parent
    pretraining_dir = base_path / dataset / "pretraining"
    
    if not pretraining_dir.exists():
        print(f"‚ùå Pasta de pr√©-treinamento n√£o encontrada: {pretraining_dir}")
        return []
    
    # Buscar por diret√≥rios de pr√©-treinamento para o modelo
    pattern = f"pretrain_{model}_*"
    runs = list(pretraining_dir.glob(pattern))
    
    if not runs:
        print(f"‚ùå Nenhum pr√©-treinamento encontrado para {model}")
        return []
    
    # Ordenar por timestamp (mais recente primeiro)
    runs.sort(key=lambda x: x.name.split('_')[-1], reverse=True)
    
    print(f"\nüìÇ Pr√©-treinamentos dispon√≠veis para {dataset}/{model}:")
    
    all_checkpoints = []
    for i, run_dir in enumerate(runs):
        print(f"\n   üóÇÔ∏è  {i+1}. {run_dir.name}")
        
        # Listar checkpoints neste run
        checkpoints = list(run_dir.glob("checkpoint_epoch_*.pt"))
        checkpoints.sort(key=lambda x: int(x.stem.split('_')[-1]))
        
        if checkpoints:
            print(f"      üìÅ Checkpoints dispon√≠veis:")
            for cp in checkpoints:
                epoch = cp.stem.split('_')[-1]
                size_mb = cp.stat().st_size / (1024*1024)
                print(f"         ‚Ä¢ √âpoca {epoch}: {cp.name} ({size_mb:.1f}MB)")
                all_checkpoints.append(cp)
        
        # Verificar se tem backbone pronto
        backbone_files = [
            run_dir / "best_pretrained_esmc.pt",
            run_dir / f"pretrained_esmc_epoch_{len(checkpoints)}.pt"
        ]
        
        available_backbones = [f for f in backbone_files if f.exists()]
        if available_backbones:
            print(f"      üß¨ Backbones prontos:")
            for bb in available_backbones:
                size_mb = bb.stat().st_size / (1024*1024)
                print(f"         ‚Ä¢ {bb.name} ({size_mb:.1f}MB)")
    
    return all_checkpoints 