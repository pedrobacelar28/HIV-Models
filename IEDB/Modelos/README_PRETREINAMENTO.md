# ğŸ§¬ PrÃ©-treinamento MLM para ESM-C em Epitopos

## ğŸ“– VisÃ£o Geral

Este sistema implementa **Masked Language Modeling (MLM)** para prÃ©-treinar modelos ESM-C especificamente em sequÃªncias de epitopos. O objetivo Ã© adaptar o modelo, que foi originalmente treinado em sequÃªncias proteicas longas, para melhor compreender padrÃµes em epitopos curtos.

### ğŸ¤” Por que PrÃ©-treinamento?

1. **DivergÃªncia de DomÃ­nio**: ESM-C foi treinado em proteÃ­nas longas, epitopos sÃ£o sequÃªncias curtas (8-15 aminoÃ¡cidos)
2. **PadrÃµes EspecÃ­ficos**: Epitopos tÃªm padrÃµes estruturais e funcionais especÃ­ficos
3. **Melhora de Performance**: PrÃ©-treinamento pode melhorar convergÃªncia e performance final

## ğŸš€ Como Usar

### OpÃ§Ã£o 1: PrÃ©-treinamento + Fine-tuning AutomÃ¡tico

```bash
python main.py \
    --dataset MHC2 \
    --model esmc_300m \
    --pretrain \
    --pretrain-epochs 10 \
    --epochs 15
```

### OpÃ§Ã£o 2: Usar Backbone JÃ¡ PrÃ©-treinado

```bash
python main.py \
    --dataset MHC2 \
    --model esmc_300m \
    --pretrained-backbone-path ./MHC2/pretraining/pretrain_esmc_300m_20241218_143022/best_pretrained_esmc.pt \
    --epochs 15
```

### OpÃ§Ã£o 3: Apenas PrÃ©-treinamento (Manual)

```python
from pretrainer import PretrainerESMC, load_sequences_from_files

# Carregar sequÃªncias
sequences = load_sequences_from_files("dados_pos.txt", "dados_neg.txt")

# Configurar prÃ©-treinador
pretrainer = PretrainerESMC(
    sequences=sequences,
    artifacts_path="./artifacts",
    epochs=10,
    batch_size=16,
    max_length=256,
    base_model="esmc_300m"
)

# Executar
pretrained_path = pretrainer.run()
```

## âš™ï¸ ParÃ¢metros de PrÃ©-treinamento

### Argumentos Principais

| ParÃ¢metro | Default | DescriÃ§Ã£o |
|-----------|---------|-----------|
| `--pretrain` | False | Habilita prÃ©-treinamento MLM |
| `--pretrain-epochs` | 10 | NÃºmero de Ã©pocas de prÃ©-treinamento |
| `--pretrain-lr` | 5e-5 | Learning rate para prÃ©-treinamento |
| `--pretrain-batch-size` | 16 | Batch size para prÃ©-treinamento |
| `--pretrain-max-length` | 512 | Comprimento mÃ¡ximo das sequÃªncias |
| `--mlm-probability` | 0.15 | Probabilidade de mascarar tokens |

### ConfiguraÃ§Ãµes Recomendadas por Dataset

#### Dataset B (CÃ©lulas B)
```bash
--pretrain-max-length 256 \
--pretrain-batch-size 16 \
--pretrain-epochs 8
```

#### Dataset MHC1 (Epitopos MHC Classe I)
```bash
--pretrain-max-length 128 \
--pretrain-batch-size 32 \
--pretrain-epochs 10
```

#### Dataset MHC2 (Epitopos MHC Classe II)
```bash
--pretrain-max-length 256 \
--pretrain-batch-size 16 \
--pretrain-epochs 12
```

## ğŸ§  Como Funciona o MLM

### 1. EstratÃ©gia de Mascaramento

Para cada sequÃªncia de epitopo:
- **15%** dos aminoÃ¡cidos sÃ£o selecionados para mascaramento
- Destes 15%:
  - **80%** sÃ£o substituÃ­dos por `<mask>`
  - **10%** sÃ£o substituÃ­dos por aminoÃ¡cido aleatÃ³rio
  - **10%** permanecem inalterados

### 2. Exemplo PrÃ¡tico

```
SequÃªncia original: FLKEKGGL
SequÃªncia mascarada: F<mask>KEKGGL
Objetivo: Prever 'L' na posiÃ§Ã£o mascarada
```

### 3. Tokens VÃ¡lidos

Apenas aminoÃ¡cidos sÃ£o mascarados:
`A, R, N, D, C, Q, E, G, H, I, L, K, M, F, P, S, T, W, Y, V`

Tokens especiais (`<bos>`, `<eos>`, `<pad>`) nunca sÃ£o mascarados.

## ğŸ“Š Estrutura de Arquivos

```
dataset/
â”œâ”€â”€ pretraining/
â”‚   â””â”€â”€ pretrain_esmc_300m_20241218_143022/
â”‚       â”œâ”€â”€ checkpoint_epoch_10.pt          # Checkpoint completo
â”‚       â”œâ”€â”€ best_pretrained_esmc.pt         # Apenas backbone (para uso)
â”‚       â””â”€â”€ pretrained_esmc_epoch_10.pt     # Backbone da Ã©poca especÃ­fica
â””â”€â”€ model/
    â””â”€â”€ [runs de fine-tuning]
```

## ğŸ”¬ Monitoramento

### Weights & Biases

O prÃ©-treinamento Ã© automaticamente logado no W&B:

- **Projeto**: `esmc-pretrain-{dataset}`
- **MÃ©tricas**: MLM loss, learning rate, gradient norm
- **FrequÃªncia**: A cada 50 steps

### MÃ©tricas Importantes

1. **MLM Loss**: Deve diminuir consistentemente
2. **Learning Rate**: Cosine schedule com warmup
3. **Gradient Norm**: Para detectar gradient explosion

## ğŸ’¡ Dicas de OtimizaÃ§Ã£o

### ğŸ¯ HiperparÃ¢metros

1. **Learning Rate**:
   - PrÃ©-treinamento: 1e-5 a 5e-5
   - Fine-tuning: Menor que prÃ©-treinamento

2. **Batch Size**:
   - Epitopos curtos: 16-32
   - Epitopos longos: 8-16

3. **Ã‰pocas**:
   - Datasets pequenos: 5-10 Ã©pocas
   - Datasets grandes: 10-20 Ã©pocas

### ğŸ“ˆ Sinais de Sucesso

- [ ] MLM loss diminui consistentemente
- [ ] NÃ£o hÃ¡ overfitting (loss validation similar ao treino)
- [ ] Fine-tuning converge mais rÃ¡pido
- [ ] Melhor F1 score final

### âš ï¸ Problemas Comuns

#### 1. MLM Loss nÃ£o diminui
- **Causa**: Learning rate muito alto
- **SoluÃ§Ã£o**: Diminuir `--pretrain-lr`

#### 2. Out of Memory
- **Causa**: Batch size muito grande
- **SoluÃ§Ã£o**: Diminuir `--pretrain-batch-size`

#### 3. ConvergÃªncia lenta
- **Causa**: Learning rate muito baixo
- **SoluÃ§Ã£o**: Aumentar `--pretrain-lr`

## ğŸ”„ Fluxo Completo

```mermaid
graph TD
    A[SequÃªncias de Epitopos] --> B[MLM Dataset]
    B --> C[Mascaramento 15%]
    C --> D[PrÃ©-treinamento ESM-C]
    D --> E[Backbone PrÃ©-treinado]
    E --> F[Fine-tuning ClassificaÃ§Ã£o]
    F --> G[Modelo Final]
```

## ğŸ“š ReferÃªncias

1. **BERT**: Devlin et al. "BERT: Pre-training of Deep Bidirectional Transformers"
2. **ESM**: Rives et al. "Biological structure and function emerge from scaling unsupervised learning"
3. **ESM-C**: Hayes et al. "Simulating 500 million years of evolution with a language model"

## ğŸ› Troubleshooting

### Erro: "mask_token_id not found"
```python
# Verificar tokenizer
from models import get_tokenizer
tokenizer = get_tokenizer("esmc_300m")
print(f"Mask token ID: {getattr(tokenizer, 'mask_token_id', 'Not found')}")
```

### Erro: "Out of memory during pretraining"
```bash
# Reduzir batch size e max length
--pretrain-batch-size 8 \
--pretrain-max-length 128
```

### Erro: "No sequences loaded"
```bash
# Verificar arquivos de dados
ls -la dataset/train_positive.txt
ls -la dataset/train_negative.txt
```

---

## ğŸ“ Suporte

Para problemas ou dÃºvidas:
1. Verifique este README
2. Execute `python exemplo_pretreinamento.py`
3. Consulte logs do W&B
4. Verifique implementaÃ§Ã£o em `pretrainer.py` 