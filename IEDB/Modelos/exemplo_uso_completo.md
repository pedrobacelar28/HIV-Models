# ğŸš€ Guia Completo: PrÃ©-treinamento + Fine-tuning ESM-C

## ğŸ“‹ Respostas Ã s Suas Perguntas

### 1. ğŸ“ **DiferenÃ§a entre Checkpoint e Backbone**

```bash
# Estrutura apÃ³s prÃ©-treinamento:
MHC2/pretraining/pretrain_esmc_600m_20241218_143022/
â”œâ”€â”€ checkpoint_epoch_2.pt           # âœ… Para CONTINUAR prÃ©-treinamento
â”œâ”€â”€ checkpoint_epoch_4.pt           # âœ… Para CONTINUAR prÃ©-treinamento  
â”œâ”€â”€ checkpoint_epoch_6.pt           # âœ… Para CONTINUAR prÃ©-treinamento
â”œâ”€â”€ checkpoint_epoch_10.pt          # âœ… Para CONTINUAR prÃ©-treinamento
â”œâ”€â”€ pretrained_esmc_epoch_10.pt     # âœ… Para FINE-TUNING
â””â”€â”€ best_pretrained_esmc.pt         # âœ… Para FINE-TUNING (melhor)
```

**Checkpoint** = Estado completo (modelo + otimizador + scheduler)  
**Backbone** = Apenas pesos do ESM-C (para classificaÃ§Ã£o)

### 2. ğŸ¯ **Fine-tuning ApÃ³s PrÃ©-treinamento**

```python
# Configure no main.py:
DEFAULT_PRETRAIN_MODE = False  # âŒ NÃ£o prÃ©-treinar
DEFAULT_PRETRAINED_BACKBONE_PATH = "./MHC2/pretraining/pretrain_esmc_600m_20241218_143022/best_pretrained_esmc.pt"  # âœ…
```

**Ou via linha de comando:**
```bash
python main.py \
    --dataset MHC2 \
    --model esmc_600m \
    --pretrained-backbone-path ./MHC2/pretraining/pretrain_esmc_600m_20241218_143022/best_pretrained_esmc.pt \
    --epochs 15
```

### 3. ğŸ”„ **Continuar PrÃ©-treinamento por Mais Ã‰pocas**

```bash
# Continuar de onde parou:
python main.py \
    --dataset MHC2 \
    --model esmc_600m \
    --resume-pretraining ./MHC2/pretraining/pretrain_esmc_600m_20241218_143022/checkpoint_epoch_10.pt \
    --pretrain-epochs 20  # Total de Ã©pocas desejadas
```

---

## ğŸ”¬ CenÃ¡rios PrÃ¡ticos

### **CenÃ¡rio A: Primeiro PrÃ©-treinamento**

```bash
# 1. PrÃ©-treinar por 10 Ã©pocas
python main.py \
    --dataset MHC2 \
    --model esmc_600m \
    --pretrain \
    --pretrain-epochs 10 \
    --pretrain-lr 5e-5 \
    --pretrain-batch-size 128

# Resultado: Salva em ./MHC2/pretraining/pretrain_esmc_600m_TIMESTAMP/
```

### **CenÃ¡rio B: Continuar PrÃ©-treinamento**

```bash
# 2. Listar checkpoints disponÃ­veis
python main.py --dataset MHC2 --model esmc_600m --list-pretraining

# 3. Continuar por mais 10 Ã©pocas (total 20)
python main.py \
    --dataset MHC2 \
    --model esmc_600m \
    --resume-pretraining ./MHC2/pretraining/pretrain_esmc_600m_TIMESTAMP/checkpoint_epoch_10.pt \
    --pretrain-epochs 20
```

### **CenÃ¡rio C: Fine-tuning com Backbone**

```bash
# 4. Fine-tuning usando backbone prÃ©-treinado
python main.py \
    --dataset MHC2 \
    --model esmc_600m \
    --pretrained-backbone-path ./MHC2/pretraining/pretrain_esmc_600m_TIMESTAMP/best_pretrained_esmc.pt \
    --epochs 15 \
    --lr 1e-5
```

---

## ğŸ¯ Workflows Recomendados

### **Workflow 1: PrÃ©-treinamento Incremental**

```bash
# Etapa 1: PrÃ©-treinar 5 Ã©pocas (teste rÃ¡pido)
python main.py --dataset MHC2 --model esmc_600m --pretrain --pretrain-epochs 5

# Verificar W&B - se loss diminuindo bem:
# Etapa 2: Continuar por mais 10 Ã©pocas (total 15)
python main.py --dataset MHC2 --model esmc_600m \
    --resume-pretraining ./MHC2/pretraining/pretrain_esmc_600m_*/checkpoint_epoch_5.pt \
    --pretrain-epochs 15

# Etapa 3: Fine-tuning
python main.py --dataset MHC2 --model esmc_600m \
    --pretrained-backbone-path ./MHC2/pretraining/pretrain_esmc_600m_*/best_pretrained_esmc.pt \
    --epochs 10
```

### **Workflow 2: AutomÃ¡tico (Recomendado)**

```bash
# Tudo em uma execuÃ§Ã£o: 10 Ã©pocas prÃ©-treino + 15 fine-tuning
python main.py \
    --dataset MHC2 \
    --model esmc_600m \
    --pretrain \
    --pretrain-epochs 10 \
    --epochs 15
```

---

## ğŸ” Comandos Ãšteis

### **Listar Modelos DisponÃ­veis**

```bash
# Listar runs de fine-tuning
python main.py --dataset MHC2 --model esmc_600m --list-runs

# Listar checkpoints de prÃ©-treinamento
python main.py --dataset MHC2 --model esmc_600m --list-pretraining
```

### **Monitoramento**

```bash
# Monitorar prÃ©-treinamento
# W&B projeto: esmc-pretrain-mhc2

# Monitorar fine-tuning  
# W&B projeto: protein-mhc2
```

---

## âš¡ ConfiguraÃ§Ãµes Otimizadas

### **Para Dataset MHC2**

```bash
# PrÃ©-treinamento otimizado
--pretrain-max-length 60 \
--pretrain-batch-size 128 \
--pretrain-lr 5e-5 \
--pretrain-epochs 10

# Fine-tuning otimizado
--batch-size 128 \
--lr 1e-5 \
--epochs 15
```

### **Para Dataset com Poucos Dados**

```bash
# PrÃ©-treinamento mais conservador
--pretrain-epochs 5 \
--pretrain-lr 1e-5 \
--pretrain-batch-size 64

# Fine-tuning com early stopping
--epochs 20  # W&B vai parar se nÃ£o melhorar
```

---

## ğŸ› SoluÃ§Ãµes para Problemas Comuns

### **1. "Out of memory" durante prÃ©-treinamento**

```bash
# Reduzir batch size e max length
--pretrain-batch-size 32 \
--pretrain-max-length 40
```

### **2. Loss MLM nÃ£o diminui**

```bash
# Reduzir learning rate
--pretrain-lr 1e-5  # Em vez de 5e-5
```

### **3. Checkpoint nÃ£o encontrado**

```bash
# Verificar caminhos disponÃ­veis
python main.py --dataset MHC2 --model esmc_600m --list-pretraining

# Usar caminho completo
--resume-pretraining /caminho/completo/para/checkpoint_epoch_X.pt
```

### **4. Comparar performance**

```bash
# Baseline (sem prÃ©-treinamento)
python main.py --dataset MHC2 --model esmc_600m --epochs 15 --wandb-project comparison

# Com prÃ©-treinamento
python main.py --dataset MHC2 --model esmc_600m --pretrain --pretrain-epochs 10 --epochs 15 --wandb-project comparison
```

---

## ğŸ† Expectativas de Melhoria

### **Sinais de Sucesso**

- [ ] MLM loss diminui de ~6.0 para ~2.0-3.0
- [ ] Fine-tuning converge em menos Ã©pocas
- [ ] F1 score 2-5% maior que baseline
- [ ] Precision melhor (menos falsos positivos)
- [ ] Treinamento mais estÃ¡vel

### **Quando Usar PrÃ©-treinamento**

âœ… **Use quando:**
- Dataset de epitopos < 10k sequÃªncias
- Epitopos muito curtos (8-12 aminoÃ¡cidos)
- Quero melhor precision
- Tenho tempo para experimentar

âŒ **NÃ£o use quando:**
- Dataset muito grande (>100k)
- Epitopos longos (>20 aminoÃ¡cidos)
- Deadline apertado
- Baseline jÃ¡ estÃ¡ excelente

---

## ğŸ“Š Monitoramento AvanÃ§ado

### **MÃ©tricas de PrÃ©-treinamento (W&B)**

```python
# train/loss - deve diminuir consistentemente
# train/learning_rate - schedule cosine
# train/step - progresso total
```

### **ComparaÃ§Ã£o Final**

| MÃ©trica | Baseline | Com PrÃ©-treino | Melhoria |
|---------|----------|----------------|----------|
| F1 Score | 0.75 | 0.78 | +3% |
| Precision | 0.70 | 0.76 | +6% |
| ConvergÃªncia | 12 Ã©pocas | 8 Ã©pocas | -33% |

---

**ğŸ¯ Resumo: Suas perguntas foram respondidas e agora vocÃª tem um sistema completo para experimentar com prÃ©-treinamento incremental!** 