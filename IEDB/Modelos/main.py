import argparse
import os
from pathlib import Path
from config import get_config
from trainer import Trainer
import wandb
import yaml

# ================================================================== #
# ğŸ”§ CONFIGURAÃ‡ÃƒO DE DEFAULTS (EDITE AQUI PARA FACILIZAR O USO)     #
# ================================================================== #
# Edite estes valores para evitar usar argumentos na linha de comando

# â”€â”€â”€â”€â”€â”€ ConfiguraÃ§Ã£o bÃ¡sica â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_DATASET = "MHC2"                    # OpÃ§Ãµes: "B", "MHC1", "MHC2"
DEFAULT_MODEL = "esmc_600m"                  # OpÃ§Ãµes: "esmc_300m", "esmc_600m", "esm2_t33_650M_UR50D", "esm2_t36_3B_UR50D"
DEFAULT_VIRUS_TYPE = "Base"                  # OpÃ§Ãµes: "Base", "Tudo", "Virus", "Lent", "Retro"
DEFAULT_EVAL_MODE = False                    # True = apenas avaliaÃ§Ã£o, False = treinamento

# â”€â”€â”€â”€â”€â”€ HiperparÃ¢metros de treinamento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_EPOCHS = 5                         # NÃºmero de Ã©pocas (max_iter para early terminate)
DEFAULT_LEARNING_RATE = 5e-5              # Taxa de aprendizado
DEFAULT_WEIGHT_DECAY = 0.00                 # Decaimento de peso (L2 regularization)
DEFAULT_BATCH_SIZE = 128                   # None = usar padrÃ£o do modelo, ou especificar valor
DEFAULT_MAX_LENGTH = None                   # None = usar padrÃ£o do modelo, ou especificar valor

# â”€â”€â”€â”€â”€â”€ ConfiguraÃ§Ã£o do modelo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_DROPOUT = 0.0                      # None = usar padrÃ£o do modelo, ou especificar (ex: 0.3)
DEFAULT_FREEZE_BACKBONE = False             # True = congelar encoder, False = treinar tudo

# â”€â”€â”€â”€â”€â”€ Pesos da loss function para precision â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_POS_CLASS_WEIGHT = 3.0             # Peso aplicado Ã  classe negativa (> 1.0 melhora precision)
DEFAULT_LOSS_WEIGHT_MULTIPLIER = 1.0       # Multiplicador escalar adicional

# â”€â”€â”€â”€â”€â”€ Para modo AVALIAÃ‡ÃƒO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_RUN_NAME = None                     # Ex: "B_Base_esmc_300m_20241218_143022" ou None para mais recente

# â”€â”€â”€â”€â”€â”€ Intervalos de salvamento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_EVAL_INTERVAL = 1                  # Avaliar a cada N Ã©pocas
DEFAULT_SAVE_INTERVAL = 1                  # Salvar checkpoint a cada N Ã©pocas

# â”€â”€â”€â”€â”€â”€ ConfiguraÃ§Ã£o W&B â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_WANDB_PROJECT = None               # None = automÃ¡tico (protein-{dataset}), ou especificar
DEFAULT_WANDB_ENTITY = None                # None = conta padrÃ£o, ou especificar organizaÃ§Ã£o

# â”€â”€â”€â”€â”€â”€ Reprodutibilidade â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_SEED = 42                          # Semente para reprodutibilidade

# â”€â”€â”€â”€â”€â”€ W&B Hyperparameter Sweep â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_SWEEP_MODE = False                 # True = ativar sweep para otimizaÃ§Ã£o automÃ¡tica

# â”€â”€â”€â”€â”€â”€ PrÃ©-treinamento MLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_PRETRAIN_MODE = False              # True = executar prÃ©-treinamento MLM
DEFAULT_PRETRAIN_EPOCHS = 15               # Ã‰pocas de prÃ©-treinamento
DEFAULT_PRETRAIN_LR = 5e-4                 # Learning rate para prÃ©-treinamento
DEFAULT_PRETRAIN_BATCH_SIZE = 128           # Batch size para prÃ©-treinamento
DEFAULT_PRETRAIN_MAX_LENGTH = 60          # Comprimento mÃ¡ximo para epitopos
DEFAULT_MLM_PROBABILITY = 0.15             # Probabilidade de mascaramento MLM
DEFAULT_PRETRAINED_BACKBONE_PATH = "/home/ubuntu/guilherme.evangelista/HIV-Models/IEDB/Modelos/MHC2/model/pretraining/pretrain_esmc_600m_20250527_031230/best_pretrained_esmc.pt"   # Caminho para backbone prÃ©-treinado
DEFAULT_RESUME_PRETRAINING_BACKBONE_PATH = None

# ================================================================== #


def list_available_runs(dataset, model, virus_type):
    """Lista runs disponÃ­veis para avaliaÃ§Ã£o"""
    base_path = Path(__file__).parent
    model_dir = base_path / dataset / "model"
    
    if not model_dir.exists():
        print(f"âŒ Pasta de modelos nÃ£o encontrada: {model_dir}")
        return []
    
    # PadrÃ£o: {dataset}_{virus_type}_{model}_{timestamp}
    pattern = f"{dataset}_{virus_type}_{model}_*"
    runs = list(model_dir.glob(pattern))
    
    if not runs:
        print(f"âŒ Nenhum modelo encontrado para {dataset}_{virus_type}_{model}")
        return []
    
    # Ordenar por timestamp (mais recente primeiro)
    runs.sort(key=lambda x: x.name.split('_')[-1], reverse=True)
    
    print(f"\nğŸ“‚ Runs disponÃ­veis para {dataset}_{virus_type}_{model}:")
    for i, run in enumerate(runs):
        timestamp = run.name.split('_')[-1]
        best_model = run / "best_model.pt"
        status = "âœ… best_model.pt" if best_model.exists() else "âš ï¸  sem best_model.pt"
        print(f"   {i+1}. {run.name} ({status})")
    
    return runs


def parse_args():
    parser = argparse.ArgumentParser(description="Training or Evaluation mode for HIV protein models")
    
    # â”€â”€â”€â”€â”€â”€ Modo de execuÃ§Ã£o â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parser.add_argument(
        "--eval", action="store_true", default=DEFAULT_EVAL_MODE,
        help="Run in evaluation mode (default: False)"
    )
    
    # â”€â”€â”€â”€â”€â”€ ConfiguraÃ§Ã£o bÃ¡sica â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parser.add_argument(
        "--dataset", type=str, default=DEFAULT_DATASET, choices=["B", "MHC1", "MHC2"],
        help=f"Dataset to use: B, MHC1, or MHC2 (default: {DEFAULT_DATASET})"
    )
    
    parser.add_argument(
        "--model", type=str, default=DEFAULT_MODEL,
        choices=["esmc_300m", "esmc_600m", "esm2_t33_650M_UR50D", "esm2_t36_3B_UR50D"],
        help=f"Model to use (default: {DEFAULT_MODEL})"
    )
    
    parser.add_argument(
        "--virus-type", type=str, default=DEFAULT_VIRUS_TYPE, 
        choices=["Base", "Tudo", "Virus", "Lent", "Retro"],
        help=f"Type of virus data to use (default: {DEFAULT_VIRUS_TYPE}). 'Base' uses files without suffix"
    )
    
    # â”€â”€â”€â”€â”€â”€ HiperparÃ¢metros de treinamento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parser.add_argument(
        "--epochs", type=int, default=DEFAULT_EPOCHS,
        help=f"Number of training epochs (default: {DEFAULT_EPOCHS})"
    )
    
    parser.add_argument(
        "--lr", "--learning-rate", type=float, default=DEFAULT_LEARNING_RATE,
        help=f"Learning rate (default: {DEFAULT_LEARNING_RATE})"
    )
    
    parser.add_argument(
        "--weight-decay", type=float, default=DEFAULT_WEIGHT_DECAY,
        help=f"Weight decay for L2 regularization (default: {DEFAULT_WEIGHT_DECAY})"
    )
    
    parser.add_argument(
        "--batch-size", type=int, default=DEFAULT_BATCH_SIZE,
        help=f"Batch size (default: auto based on model)"
    )
    
    parser.add_argument(
        "--max-length", type=int, default=DEFAULT_MAX_LENGTH,
        help=f"Maximum sequence length (default: auto based on model)"
    )
    
    # â”€â”€â”€â”€â”€â”€ ConfiguraÃ§Ã£o do modelo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parser.add_argument(
        "--dropout", type=float, default=DEFAULT_DROPOUT,
        help=f"Dropout rate (default: auto based on model)"
    )
    
    parser.add_argument(
        "--freeze-backbone", action="store_true", default=DEFAULT_FREEZE_BACKBONE,
        help=f"Freeze backbone weights (default: {DEFAULT_FREEZE_BACKBONE})"
    )
    
    # â”€â”€â”€â”€â”€â”€ Pesos da loss function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parser.add_argument(
        "--pos-class-weight", type=float, default=DEFAULT_POS_CLASS_WEIGHT,
        help=f"Weight for negative class to improve precision (default: {DEFAULT_POS_CLASS_WEIGHT})"
    )
    
    parser.add_argument(
        "--loss-weight-multiplier", type=float, default=DEFAULT_LOSS_WEIGHT_MULTIPLIER,
        help=f"Multiplier for loss weights (default: {DEFAULT_LOSS_WEIGHT_MULTIPLIER})"
    )
    
    # â”€â”€â”€â”€â”€â”€ Intervalos e salvamento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parser.add_argument(
        "--eval-interval", type=int, default=DEFAULT_EVAL_INTERVAL,
        help=f"Evaluate every N epochs (default: {DEFAULT_EVAL_INTERVAL})"
    )
    
    parser.add_argument(
        "--save-interval", type=int, default=DEFAULT_SAVE_INTERVAL,
        help=f"Save checkpoint every N epochs (default: {DEFAULT_SAVE_INTERVAL})"
    )
    
    # â”€â”€â”€â”€â”€â”€ Weights & Biases â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parser.add_argument(
        "--wandb-project", type=str, default=DEFAULT_WANDB_PROJECT,
        help=f"W&B project name (default: auto generated)"
    )
    
    parser.add_argument(
        "--wandb-entity", type=str, default=DEFAULT_WANDB_ENTITY,
        help=f"W&B entity/organization (default: None)"
    )
    
    # â”€â”€â”€â”€â”€â”€ Reprodutibilidade â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parser.add_argument(
        "--seed", type=int, default=DEFAULT_SEED,
        help=f"Random seed for reproducibility (default: {DEFAULT_SEED})"
    )
    
    # â”€â”€â”€â”€â”€â”€ W&B Hyperparameter Sweep â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parser.add_argument(
        "--sweep", action="store_true", default=DEFAULT_SWEEP_MODE,
        help=f"Enable W&B hyperparameter sweep (default: {DEFAULT_SWEEP_MODE})"
    )
    
    # â”€â”€â”€â”€â”€â”€ PrÃ©-treinamento MLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parser.add_argument(
        "--pretrain", action="store_true", default=DEFAULT_PRETRAIN_MODE,
        help=f"Run MLM pretraining before fine-tuning (default: {DEFAULT_PRETRAIN_MODE})"
    )
    
    parser.add_argument(
        "--pretrain-epochs", type=int, default=DEFAULT_PRETRAIN_EPOCHS,
        help=f"Total number of pretraining epochs desired (default: {DEFAULT_PRETRAIN_EPOCHS}). When resuming, this is the final epoch number."
    )
    
    parser.add_argument(
        "--pretrain-lr", type=float, default=DEFAULT_PRETRAIN_LR,
        help=f"Learning rate for pretraining (default: {DEFAULT_PRETRAIN_LR})"
    )
    
    parser.add_argument(
        "--pretrain-batch-size", type=int, default=DEFAULT_PRETRAIN_BATCH_SIZE,
        help=f"Batch size for pretraining (default: {DEFAULT_PRETRAIN_BATCH_SIZE})"
    )
    
    parser.add_argument(
        "--pretrain-max-length", type=int, default=DEFAULT_PRETRAIN_MAX_LENGTH,
        help=f"Max length for pretraining epitopes (default: {DEFAULT_PRETRAIN_MAX_LENGTH})"
    )
    
    parser.add_argument(
        "--mlm-probability", type=float, default=DEFAULT_MLM_PROBABILITY,
        help=f"MLM masking probability (default: {DEFAULT_MLM_PROBABILITY})"
    )
    
    parser.add_argument(
        "--pretrained-backbone-path", type=str, default=DEFAULT_PRETRAINED_BACKBONE_PATH,
        help=f"Path to pretrained backbone weights (default: None)"
    )
    
    parser.add_argument(
        "--resume-pretraining", type=str, default=DEFAULT_RESUME_PRETRAINING_BACKBONE_PATH,
        help="Path to checkpoint to resume pretraining from"
    )
    
    # â”€â”€â”€â”€â”€â”€ AvaliaÃ§Ã£o especÃ­fica â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parser.add_argument(
        "--step", type=int, default=None,
        help="Specific step to load for evaluation (default: melhor modelo)"
    )
    
    parser.add_argument(
        "--run-name", type=str, default=DEFAULT_RUN_NAME,
        help=f"Specific run name to evaluate (default: {DEFAULT_RUN_NAME or 'most recent'}). Use --list-runs to see available runs"
    )
    
    parser.add_argument(
        "--list-runs", action="store_true",
        help="List available runs for the specified dataset/model/virus-type and exit"
    )
    
    parser.add_argument(
        "--list-pretraining", action="store_true",
        help="List available pretraining checkpoints and backbones for the specified dataset/model and exit"
    )
    
    return parser.parse_args()


def sweep_train():
    """
    FunÃ§Ã£o para treinamento durante o sweep do W&B.
    Esta funÃ§Ã£o serÃ¡ chamada pelo sweep agent para cada combinaÃ§Ã£o de hiperparÃ¢metros.
    """
    with wandb.init() as run:
        # Pegar hiperparÃ¢metros sugeridos pelo sweep
        config_wandb = wandb.config
        
        # Usar defaults do main.py como base
        args = parse_args()
        
        # Override com hiperparÃ¢metros do sweep
        if hasattr(config_wandb, 'pos_class_weight'):
            args.pos_class_weight = config_wandb.pos_class_weight
        if hasattr(config_wandb, 'loss_weight_multiplier'):
            args.loss_weight_multiplier = config_wandb.loss_weight_multiplier
        if hasattr(config_wandb, 'lr'):
            args.lr = config_wandb.lr
        if hasattr(config_wandb, 'weight_decay'):
            args.weight_decay = config_wandb.weight_decay
        if hasattr(config_wandb, 'dropout'):
            args.dropout = config_wandb.dropout
        
        # Get configuration bÃ¡sica
        config = get_config(
            dataset_type=args.dataset,
            model_type=args.model,
            virus_type=args.virus_type
        )
        
        # Apply hiperparÃ¢metros
        config["eval"] = False  # Sempre treinamento no sweep
        
        # Usar Ã©pocas fixas do args - W&B controlarÃ¡ early terminate externamente
        config["epochs"] = args.epochs
            
        config["seed"] = args.seed
        config["lr"] = args.lr
        config["weight_decay"] = args.weight_decay
        config["pos_class_weight"] = args.pos_class_weight
        config["loss_weight_multiplier"] = args.loss_weight_multiplier
        config["eval_interval"] = args.eval_interval
        config["save_interval"] = args.save_interval
        
        # Override configuraÃ§Ãµes especÃ­ficas para sweep
        if args.batch_size is not None:
            config["batch_size"] = args.batch_size
        if args.max_length is not None:
            config["max_length"] = args.max_length
        if args.dropout is not None:
            config["dropout"] = args.dropout
        config["freeze_backbone"] = args.freeze_backbone
        
        # W&B config
        if args.wandb_project is not None:
            config["project"] = args.wandb_project
        config["entity"] = args.wandb_entity
        
        # NOVO: Adicionar pretrained_backbone_path ao sweep
        if args.pretrained_backbone_path:
            config["pretrained_backbone_path"] = args.pretrained_backbone_path
        
        # Gerar run_name especÃ­fico para sweep
        sweep_id = wandb.run.sweep_id if wandb.run.sweep_id else "manual"
        run_id = wandb.run.id
        config["run_name"] = f"sweep_{sweep_id}_{run_id}"
        
        print(f"\nğŸ¯ SWEEP RUN - HiperparÃ¢metros testados:")
        print(f"{'='*50}")
        print(f"   Pos Class Weight: {args.pos_class_weight:.3f}")
        print(f"   Loss Weight Multiplier: {args.loss_weight_multiplier:.3f}")
        print(f"   Learning Rate: {args.lr:.6f}")
        print(f"   Weight Decay: {args.weight_decay:.6f}")
        print(f"   Dropout: {config.get('dropout', 'auto')}")
        print(f"   Max Ã‰pocas: {config['epochs']} (W&B pode parar antes)")
        
        # Mostrar se estÃ¡ usando backbone prÃ©-treinado
        if args.pretrained_backbone_path:
            backbone_name = Path(args.pretrained_backbone_path).name
            print(f"   ğŸ§¬ Backbone prÃ©-treinado: {backbone_name}")
        else:
            print(f"   ğŸ§¬ Backbone: Original (sem prÃ©-treinamento)")
        
        print(f"{'='*50}")
        
        # Executar treinamento
        trainer = Trainer(**config)
        trainer.run()


def start_sweep():
    """
    Inicia um novo sweep do W&B.
    """
    # Carregar configuraÃ§Ã£o do sweep
    sweep_config_path = Path(__file__).parent / "sweep_config.yaml"
    
    if not sweep_config_path.exists():
        print(f"âŒ Arquivo de configuraÃ§Ã£o do sweep nÃ£o encontrado: {sweep_config_path}")
        print("ğŸ’¡ Certifique-se de que o arquivo sweep_config.yaml existe")
        return
    
    with open(sweep_config_path, 'r') as f:
        sweep_config = yaml.safe_load(f)
    
    # Obter configuraÃ§Ãµes do usuÃ¡rio
    args = parse_args()
    
    # Determinar nome do projeto W&B
    project_name = args.wandb_project if args.wandb_project else f"protein-{args.dataset.lower()}-sweep"
    
    print(f"\nğŸš€ INICIANDO W&B SWEEP")
    print(f"{'='*60}")
    print(f"ğŸ“ Dataset: {args.dataset}")
    print(f"ğŸ§  Modelo: {args.model}")
    print(f"ğŸ¦  Tipo vÃ­rus: {args.virus_type}")
    print(f"ğŸ“Š Projeto W&B: {project_name}")
    print(f"ğŸ¯ MÃ©trica: {sweep_config['metric']['name']}")
    print(f"ğŸ”„ Max runs: {sweep_config.get('run_cap', 'unlimited')}")
    print(f"{'='*60}")
    
    # Criar sweep
    sweep_id = wandb.sweep(
        sweep_config, 
        project=project_name,
        entity=args.wandb_entity
    )
    
    print(f"\nâœ… Sweep criado com ID: {sweep_id}")
    print(f"ğŸŒ Acompanhe em: https://wandb.ai/{args.wandb_entity or 'sua-conta'}/{project_name}/sweeps/{sweep_id}")
    print(f"\nğŸ¤– Iniciando sweep agent...")
    
    # Executar sweep
    wandb.agent(sweep_id, sweep_train, count=sweep_config.get('run_cap', None))
    
    print(f"\nğŸ Sweep concluÃ­do!")


def main():
    args = parse_args()
    
    # â”€â”€â”€â”€â”€â”€ Modo Sweep â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.sweep:
        start_sweep()
        return
    
    # Se pediu para listar runs, faz isso e sai
    if args.list_runs:
        list_available_runs(args.dataset, args.model, args.virus_type)
        return
    
    # Se pediu para listar prÃ©-treinamentos, faz isso e sai
    if args.list_pretraining:
        from pretrainer import list_pretraining_checkpoints
        list_pretraining_checkpoints(args.dataset, args.model)
        return
    
    # Get configuration with specified parameters (apenas caminhos e estruturas bÃ¡sicas)
    config = get_config(
        dataset_type=args.dataset,
        model_type=args.model,
        virus_type=args.virus_type
    )

    # Se estÃ¡ em modo avaliaÃ§Ã£o e especificou um run
    if args.eval and args.run_name:
        # Usar o run especÃ­fico (seja do default ou do argumento)
        config["run_name"] = args.run_name
        print(f"ğŸ¯ Usando run especÃ­fico: {args.run_name}")
    elif args.eval:
        # Modo avaliaÃ§Ã£o sem run especÃ­fico - buscar o mais recente
        runs = list_available_runs(args.dataset, args.model, args.virus_type)
        if not runs:
            print("âŒ Nenhum modelo treinado encontrado!")
            print("ğŸ’¡ Dica: Primeiro treine um modelo com: python main.py")
            return
        
        # Usar o mais recente
        config["run_name"] = runs[0].name
        print(f"ğŸ¯ Usando run mais recente: {runs[0].name}")

    # â”€â”€â”€â”€â”€â”€ Override config com hiperparÃ¢metros do main.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # BÃ¡sicos
    config["eval"] = args.eval
    config["step"] = args.step
    config["epochs"] = args.epochs
    config["seed"] = args.seed
    
    # HiperparÃ¢metros de treinamento
    config["lr"] = args.lr
    config["weight_decay"] = args.weight_decay
    
    # Override batch_size e max_length se especificados
    if args.batch_size is not None:
        config["batch_size"] = args.batch_size
    if args.max_length is not None:
        config["max_length"] = args.max_length
    
    # ConfiguraÃ§Ã£o do modelo
    if args.dropout is not None:
        config["dropout"] = args.dropout
    config["freeze_backbone"] = args.freeze_backbone
    
    # Pesos da loss function
    config["pos_class_weight"] = args.pos_class_weight
    config["loss_weight_multiplier"] = args.loss_weight_multiplier
    
    # Intervalos
    config["eval_interval"] = args.eval_interval
    config["save_interval"] = args.save_interval
    
    # Weights & Biases
    if args.wandb_project is not None:
        config["project"] = args.wandb_project
    config["entity"] = args.wandb_entity
    
    # â”€â”€â”€â”€â”€â”€ PrÃ©-treinamento MLM (se habilitado) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pretrained_backbone_path = args.pretrained_backbone_path
    
    if (args.pretrain or args.resume_pretraining) and not args.eval and args.model.startswith("esmc"):
        if args.resume_pretraining:
            print(f"\nğŸ”„ CONTINUANDO PRÃ‰-TREINAMENTO MLM")
        else:
            print(f"\nğŸ§¬ EXECUTANDO PRÃ‰-TREINAMENTO MLM")
        print(f"{'='*60}")
        
        from pretrainer import PretrainerESMC, load_sequences_from_files
        
        # Carregar sequÃªncias de treino para prÃ©-treinamento
        sequences = load_sequences_from_files(config["train_pos"], config["train_neg"])
        
        if len(sequences) < 100:
            print(f"âš ï¸  Poucas sequÃªncias para prÃ©-treinamento ({len(sequences)})")
            print(f"ğŸ’¡ Recomendamos pelo menos 1000 sequÃªncias para bons resultados")
        
        # Configurar prÃ©-treinador
        pretrainer_config = {
            "sequences": sequences,
            "artifacts_path": config["artifacts_path"],
            "lr": args.pretrain_lr,
            "weight_decay": args.weight_decay,
            "batch_size": args.pretrain_batch_size,
            "max_length": args.pretrain_max_length,
            "epochs": args.pretrain_epochs,
            "base_model": args.model,
            "mlm_probability": args.mlm_probability,
            "project": f"esmc-pretrain-{args.dataset.lower()}",
            "entity": args.wandb_entity,
            "seed": args.seed,
            "resume_from_checkpoint": args.resume_pretraining,
        }
        
        if args.resume_pretraining:
            print(f"ğŸ”„ Continuando prÃ©-treinamento de:")
            print(f"   ğŸ“ {args.resume_pretraining}")
        else:
            print(f"ğŸš€ Iniciando prÃ©-treinamento:")
        
        print(f"   ğŸ“Š {len(sequences)} sequÃªncias")
        print(f"   ğŸ• {args.pretrain_epochs} Ã©pocas")
        print(f"   ğŸ“ Max length: {args.pretrain_max_length}")
        print(f"   ğŸ­ MLM prob: {args.mlm_probability}")
        print(f"   ğŸ“š Batch size: {args.pretrain_batch_size}")
        print(f"   ğŸ¯ Learning rate: {args.pretrain_lr}")
        
        # Executar prÃ©-treinamento
        pretrainer = PretrainerESMC(**pretrainer_config)
        pretrained_backbone_path = pretrainer.run()
        
        print(f"\nâœ… PrÃ©-treinamento concluÃ­do!")
        print(f"ğŸ¯ Backbone prÃ©-treinado salvo em: {pretrained_backbone_path}")
        
        if not args.resume_pretraining:
            print(f"ğŸ’¡ Usando este backbone para fine-tuning...")
        
    elif (args.pretrain or args.resume_pretraining) and args.model.startswith("esm2"):
        print(f"âš ï¸  PrÃ©-treinamento MLM ainda nÃ£o suportado para ESM2")
        print(f"ğŸ’¡ Use modelos ESM-C (esmc_300m ou esmc_600m) para prÃ©-treinamento")
    
    # Adicionar caminho do backbone prÃ©-treinado ao config
    if pretrained_backbone_path:
        config["pretrained_backbone_path"] = pretrained_backbone_path
    
    # â”€â”€â”€â”€â”€â”€ Print configuration for confirmation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\nğŸ§¬ CONFIGURAÃ‡ÃƒO COMPLETA:")
    print(f"{'='*60}")
    print(f"ğŸ“ Dataset & Modelo:")
    print(f"   Dataset: {args.dataset}")
    print(f"   Modelo: {args.model}")  
    print(f"   Tipo de vÃ­rus: {args.virus_type}")
    print(f"   Modo: {'AvaliaÃ§Ã£o' if args.eval else 'Treinamento'}")
    
    print(f"\nâš™ï¸  HiperparÃ¢metros de Treinamento:")
    print(f"   Ã‰pocas: {args.epochs}")
    print(f"   Learning Rate: {args.lr}")
    print(f"   Weight Decay: {args.weight_decay}")
    print(f"   Batch Size: {config['batch_size']}")
    print(f"   Max Length: {config['max_length']}")
    
    print(f"\nğŸ§  ConfiguraÃ§Ã£o do Modelo:")
    print(f"   Dropout: {config.get('dropout', 'auto')}")
    print(f"   Freeze Backbone: {args.freeze_backbone}")
    
    print(f"\nâš–ï¸  Pesos da Loss Function:")
    print(f"   Pos Class Weight: {args.pos_class_weight}")
    print(f"   Loss Weight Multiplier: {args.loss_weight_multiplier}")
    print(f"   ğŸ’¡ Efeito: {'Melhor precision' if args.pos_class_weight > 1.0 else 'PadrÃ£o' if args.pos_class_weight == 1.0 else 'Melhor recall'}")
    
    print(f"\nğŸ“Š Intervalos:")
    print(f"   Eval Interval: {args.eval_interval}")
    print(f"   Save Interval: {args.save_interval}")
    
    print(f"\nğŸ“ˆ Weights & Biases:")
    print(f"   Project: {config['project']}")
    print(f"   Entity: {config.get('entity', 'None')}")
    
    print(f"\nğŸ“‚ Caminhos:")
    print(f"   Run name: {config['run_name']}")
    print(f"   Arquivos positivos: {config['train_pos'].split('/')[-1]}")
    print(f"   Arquivos negativos: {config['train_neg'].split('/')[-1]}")
    print(f"   Pasta de saÃ­da: {config['artifacts_path']}")
    print(f"{'='*60}")

    # Run trainer
    Trainer(**config).run()


if __name__ == "__main__":
    main()
